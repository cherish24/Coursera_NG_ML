# 多类别分类问题   

该部分使用之前的逻辑回归模型对手写数字0~9的识别。手写数字使用20*20像素的灰度图片，因此特征变量x的个数为400个。由于Octave\MATLAB的下标由1开始，因此我们使用10表示手写数字中的0。   

**任务一 可视化数据**    

随机选择100条数据，通过绘图函数将数据可视化。其在代码已在ex3.m和displayData.m文件中写好，只需在Octave中运行该部分代码即可，其结果如下：       

![](http://upload-images.jianshu.io/upload_images/5983416-2a31321cc74f4d70.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**任务二 向量化后的代价函数和梯度下降算法**    

向量化后的代价函数和梯度下降算法的公式分别为：     

代价函数J(θ)

![](http://upload-images.jianshu.io/upload_images/5983416-0cd860778c9dab9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

梯度下降算法

![](http://upload-images.jianshu.io/upload_images/5983416-cf0b8a41507d3f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此，我们根据上述公式在lrCostFunction.m文件中补充的代码如下：    

```
J = (-y' * log(sigmoid(X * theta)) - (1-y)' * log(1 - sigmoid(X * theta))) / m;
grad = (X' * (sigmoid(X * theta) - y)) / m;
```

在编程作业（二）中，我们也在相应文件中补充过相同代码。其中，梯度下降算法我们只补充了迭代表达式，这是为了在此之后便于调用高级选择算法。（注：在编程作业（二）中未曾说明，此处补充说明一下。）    

**任务三 正则化的代价函数和梯度下降算法**    

在任务二的基础上，我们将代码修改为正则化的代价函数和梯度下降算法，其代码如下：       

```
temp = theta;
temp(1) = 0;
J = ((-y' * log(sigmoid(X * theta)) - (1-y)' * log(1 - sigmoid(X * theta))) / m) + (lambda / (2 * m) * (temp' * temp));
grad = ((X' * (sigmoid(X * theta) - y)) / m) + ((lambda / m) * temp);
```     

这时，我们运行一下该部分代码，其结果如下：       

```
Testing lrCostFunction() with regularization
Cost: 2.534819
Expected cost: 2.534819
Gradients:
 0.146561
 -0.548558
 0.724722
 1.398003
Expected gradients:
 0.146561
 -0.548558
 0.724722
 1.398003
```         

**任务四 采用一对多方法分类**     

在多类别分类问题中，我们的训练集中有个多个类别，这时可以利用一对多的分类思想将多类别分类问题转化为二元分类问题，该方法也称为一对多分类方法。具体请参考[逻辑回归（三）](http://www.jianshu.com/p/1f788114afc1)或查阅相关文档。    

此处我们通过使用高级选择算法来计算出使得代价函数J(θ)最小化的参数θ的值。

我们在predictOneVsAll.m文件中补充代码如下：        

```
initial_theta = zeros(n + 1, 1);
options = optimset('GradObj', 'on', 'MaxIter', 50);

for c=1:num_labels
	all_theta(c, :) = fmincg(@(t)(lrCostFunction(t, X, (y == c), lambda)), initial_theta, options);
end
```      

其中，我们使用fmincg函数替代之前我们使用的fminunc函数，是为了提高处理大量参数的运行效率。all_theta矩阵中每一行代表在行数顺序下的参数θ的值。              

该部分代码运行结果如下：       

```
Training One-vs-All Logistic Regression...
Iteration    50 | Cost: 1.393306e-002
Iteration    50 | Cost: 5.725244e-002
Iteration    50 | Cost: 6.370451e-002
Iteration    50 | Cost: 3.584608e-002
Iteration    50 | Cost: 6.184004e-002
Iteration    50 | Cost: 2.187578e-002
Iteration    50 | Cost: 3.550127e-002
Iteration    50 | Cost: 8.587966e-002
Iteration    50 | Cost: 7.956057e-002
Iteration    50 | Cost: 9.989593e-003
```        

**任务五 结果预测**    

我们利用之前的计算出的参数参数θ的值，对结果进行预测。      

![](http://upload-images.jianshu.io/upload_images/5983416-b6a15e0801dd4103.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

根据逻辑函数g(z)可知：      
- 当z≥0.5时，我们可以预测y=1  
- 当z﹤0.5时，我们可以预测y=0      

由此，我们在predictOneVsAll.m文件中补充如下代码：        

```
[a, p] = max(sigmoid( X * all_theta'), [], 2);
```         

其中a代表矩阵中每行最大的值，p代表矩阵中每行最大的值在每一行的位置，即预测的数字。        

该部分运行结果如下：      

```
Training Set Accuracy: 95.060000
```       

该结果表明，我们所预测的结果精度大约为95.06%。         

# 神经网络模型   

我们使用神经网络模型对手写数字进行识别，由于使用20*20像素的灰度图片，因此输入层的激活单元的个数为400个，隐藏层的激活单元个数为25，输出层的激活单元个数为10个。      

![](http://upload-images.jianshu.io/upload_images/5983416-6a8f897cac945747.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**任务 预测结果**    

假设函数h~Θ~(x)：  

![](http://upload-images.jianshu.io/upload_images/5983416-cbe8355c4939f4fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，逻辑函数g(z)图像为：    

![](http://upload-images.jianshu.io/upload_images/5983416-b6a15e0801dd4103.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

根据逻辑函数g(z)可知：      
- 当z≥0.5时，我们可以预测y=1  
- 当z﹤0.5时，我们可以预测y=0      

因此，我们在predict.m文件中补充如下代码：      

```
X = [ones(m, 1) X];
a2 = sigmoid(X * Theta1');

a2 = [ones(m, 1) a2];
a3 = sigmoid(a2 * Theta2');

[a, p] = max(a3, [], 2);
```        

该部分运行结果为：      

```
Training Set Accuracy: 97.520000
```
