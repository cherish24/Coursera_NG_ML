###### 主成分分析问题公式  

主成分分析问题：  

- 将n维数据降为k维
- 找到向量u^(1)^, u^(2)^, ···, u^(k)^使得其投影误差最小化

对此，我们引入主成分分析法（Principal Component Analysis，简称PCA），该方法也是常见的降维方法。  

主成分分析法（PCA）：寻找一个低维的面，使得投影误差的平方和最小化。  

![](http://upload-images.jianshu.io/upload_images/5983416-b6ffce6e0d770eea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注：别因为上图类似于线性回归，就认为主成分分析法与线性回归一样。实际上，主成分分析法是最小化投影误差，而线性回归是最小化预测结果误差。   

###### 主成分分析算法  

假设数据集为{x^(1)^, x^(2)^, ···, x^(n)^}，我们希望将其从n维降为K维：   

1. 对数据集进行特征缩放和均值归一化  
2. 计算协方差矩阵（Covariance Matrix）：  

![协方差矩阵](http://upload-images.jianshu.io/upload_images/5983416-21b60d8131fdc639.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

3. 计算协方差矩阵的特征向量（Eigenvector）：[U, S, V] = svd(Sigma);   

其中，svd()函数是Octave或MATLAB中的奇异值分解（Singular Value Decomposition）函数。  

通过svd()函数我们可得到矩阵U，该矩阵是由数据间最小投影误差的方向向量构成的。我们要将n维数据集降为K维，只需在矩阵U中得到一个n*K的矩阵即可，该矩阵我们用U~reduce~表示，然后利用如下公式计算处新的特征向量z^(i)^：  

![](http://upload-images.jianshu.io/upload_images/5983416-415ad6173e420e93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注：此处X∈R^n^，即不包括x~0~=1。
