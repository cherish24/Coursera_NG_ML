###### 模型表达Ⅰ（Model Representation Ⅰ）   

为了构建神经网络模型，我们需要借鉴大脑中的神经系统。每一个神经元都可以作为一个处理单元（Processing Unit）或神经核（Nucleus），其拥有众多用于输入的树突（Dendrite）和用于输出的轴突（Axon），其中神经元通过传递电脉冲来传递信息。   

![](http://upload-images.jianshu.io/upload_images/5983416-2f4069d61db25076.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

神经网络模型由一个个“神经元”构成，而每一个“神经元”又为一个学习模型，我们将这些“神经元”称为激活单元（Activation Unit）。     

![](http://upload-images.jianshu.io/upload_images/5983416-4eaedada538f5c27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，参数θ在神经网络中也被称为权重，假设函数h~θ~(x) = g(z)，新增的x~0~称为偏置单元（Bias Unit）。    

在神经网络模型中（以三层神经网络模型为例），第一层为输入层（Input Layer），最后一层为输出层（Output Layer），中间的这层称为隐藏层（Hidden Layer）。    

![](http://upload-images.jianshu.io/upload_images/5983416-69a263e13d0c80e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)     

我们引入如下标记用于描述神经网络模型：  
  
- a~i~^(j)^：表示第j层的第i个激活单元；  
- θ^(j)^：表示从第j层映射到第j+1层时权重矩阵。   

注：在神经网络模型中，如若第j层有s~j~个激活单元，在第j+1层有s~j+1~个激活单元，则权重矩阵θ^(j)^的维度为s~j+1~ * (s~j~+1)。因此，上图中权重矩阵θ^(1)^的维度3*4。    

对于上图所示的神经网络模型，我们可用如下数学表达式表示：     

![](http://upload-images.jianshu.io/upload_images/5983416-67cd008792189e1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在逻辑回归中，我们被限制使用数据集中的原始特征变量x，虽然我们可以通过多项式来组合这些特征，但我们仍然受到原始特征变量x的限制。     

在神经网络中，原始特征变量x只作为输入层，输出层所做出的预测结果利用的是隐藏层的特征变量，由此我们可以认为隐藏层中特征变量是通过神经网络模型学习后，将得到的新特征用于预测结果，而非使用原始特征变量x用于预测结果。

###### 补充笔记   

###### Model Representation I   

![](http://upload-images.jianshu.io/upload_images/5983416-ba393b0b40973c0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Visually, a simplistic representation looks like:  

![](http://upload-images.jianshu.io/upload_images/5983416-fb331ba91bf8c72f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Our input nodes (layer 1), also known as the "input layer", go into another node (layer 2), which finally outputs the hypothesis function, known as the "output layer".   

We can have intermediate layers of nodes between the input and output layers called the "hidden layers."   

In this example, we label these intermediate or "hidden" layer nodes a~0~^2^⋯a~n~^2^ and call them "activation units."    

![](http://upload-images.jianshu.io/upload_images/5983416-1f51797304df68cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

If we had one hidden layer, it would look like:   

![](http://upload-images.jianshu.io/upload_images/5983416-becf179a5bb49363.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

The values for each of the "activation" nodes is obtained as follows:  

![](http://upload-images.jianshu.io/upload_images/5983416-446a904d56403390.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

This is saying that we compute our activation nodes by using a 3×4 matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix Θ^(2)^ containing the weights for our second layer of nodes.   

Each layer gets its own matrix of weights, Θ^(j)^.   

The dimensions of these matrices of weights is determined as follows:  

If network has s~j~ units in layer j and s~j+1~ units in layer j+1, then Θ^(j)^ will be of dimension s~j+1~×(s~j~+1).   

The +1 comes from the addition in Θ^(j)^ of the "bias nodes," x~0~ and Θ~0~^(j)^. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation:   

![](http://upload-images.jianshu.io/upload_images/5983416-11488a369b42fe95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)     

###### 模型表达Ⅱ（Model Representation II）      

![](http://upload-images.jianshu.io/upload_images/5983416-2da71bdbe4332c96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

以此图为例，之前我们介绍其数学表达式。为了方便编码及运算，我们将其向量化。     

![](http://upload-images.jianshu.io/upload_images/5983416-7d190cfb84492508.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中：    

![](http://upload-images.jianshu.io/upload_images/5983416-d741a5d1386799b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此，我们可将之前的数学表达式改写为：     

![](http://upload-images.jianshu.io/upload_images/5983416-d06f88de7d3979de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中向量X可记为a^(1)^，则z^(2)^ = Θ^(1)^a^(1)^。由此可得，a^(2)^ = g(z^(2)^)。

此时假设函数h~θ~(x)可改写为：    

![](http://upload-images.jianshu.io/upload_images/5983416-9d2cec8f903830e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中：      

![](http://upload-images.jianshu.io/upload_images/5983416-7f0028a662346937.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###### 补充笔记        

###### Model Representation II    

To re-iterate, the following is an example of a neural network:     

![](http://upload-images.jianshu.io/upload_images/5983416-096ead19b7e5c7c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

In this section we'll do a vectorized implementation of the above functions. We're going to define a new variable z~k~^(j)^ that encompasses the parameters inside our g function. In our previous example if we replaced by the variable z for all the parameters we would get:         

![](http://upload-images.jianshu.io/upload_images/5983416-da8a2e6bcf424f8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

In other words, for layer j=2 and node k, the variable z will be:      

![](http://upload-images.jianshu.io/upload_images/5983416-59008f1f4f0ff049.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

The vector representation of x and z^j^ is:      

![](http://upload-images.jianshu.io/upload_images/5983416-7d7005e2832e68c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Setting x=a^(1)^, we can rewrite the equation as:      

![](http://upload-images.jianshu.io/upload_images/5983416-dcd1cffeb2fa949c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

We are multiplying our matrix Θ^(j−1)^ with dimensions s~j~×(n+1) (where s~j~ is the number of our activation nodes) by our vector a^(j−1)^ with height (n+1). This gives us our vector z^(j)^ with height s~j~. Now we can get a vector of our activation nodes for layer j as follows:     

![](http://upload-images.jianshu.io/upload_images/5983416-7cbe416da742dd79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Where our function g can be applied element-wise to our vector z^(j)^.   

We can then add a bias unit (equal to 1) to layer j after we have computed a^(j)^. This will be element a~0~^(j)^ and will be equal to 1. To compute our final hypothesis, let's first compute another z vector:           

![](http://upload-images.jianshu.io/upload_images/5983416-896703cbb53f74f0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

We get this final z vector by multiplying the next theta matrix after Θ^(j−1)^ with the values of all the activation nodes we just got. This last theta matrix Θ^(j)^ will have only one row which is multiplied by one column a^(j)^ so that our result is a single number. We then get our final result with:         

![](http://upload-images.jianshu.io/upload_images/5983416-e4798eac26e88abb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses.
