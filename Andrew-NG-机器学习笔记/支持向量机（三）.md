###### 使用支持向量机   

我们不推荐自己编写相关代码实现支持向量机的代价函数最小化。因此，我们可以使用现有的支持向量机的软件包，如：liblinear，libsvm等。     

现假设n表示特征变量的个数，m表示训练数据的个数，则：

- 当n>>m时，我们推荐使用不带核函数的支持向量机，即线性核函数。同时，我们也可以使用逻辑回归模型。  
- 当n较小，而m适中时（如n = 1~1000，m = 10~10000），我们推荐使用高斯核函数支持向量机。  
- 当n<<m时，我们推荐先构建或增加更多的特征变量，再使用线性核函数或逻辑回归模型。    

注：  

1. 在使用高斯核函数支持向量机时，由于原特征变量的取值范围差异较大。因此，在使用高斯核函数支持向量机之前推荐进行归一化操作。    
2. 神经网络模型或许能在以上任意情况下运行良好，但其训练速度过慢。

除了高斯核函数外，我们还有以下核函数：   

- 多项式核函数（Polynomial Kernel）
- 字符串核函数（String kernel）
- 卡方核函数（ chi-square kernel）
- 直方图交集核函数（histogram intersection kernel）
······  

注：并不是所有的相似函数都能够成为有效的核函数。只有满足Mercer's Theorem的核函数才能被支持向量机的优化软件包正确处理。  

Question:   
Suppose you are trying to decide among a few different choices of kernel and are also choosing parameters such as C, σ^2^, etc. How should you make the choice?   

A. Choose whatever performs best on the training data.  
B. Choose whatever performs best on the cross-validation data.  
C. Choose whatever performs best on the test data.   
D. Choose whatever gives the largest SVM margin.   

该问题的正确答案为B。

**多分类问题**   

我们可以使用之前介绍的一对多方法来解决该问题，我们也使用支持向量机内置的多类分类问题的软件包来解决该问题。
