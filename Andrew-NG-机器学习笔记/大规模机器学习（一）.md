###### 大型数据集的学习  

It's not who has the best algorithm that wins. It's who has the most data.  

在机器学习中，决定因素往往不是算法而是数据集的大小。正如我们之前所学习的欠拟合问题，我们增加数据往往能帮助我们获得更为满意的结果。 

但大型数据集的学习都有些独特的问题，尤其是计算问题。  

现假设数据集m=100000000，我们想利用该数据集训练一个线性回归或者逻辑回归模型，并使用梯度下降算法最优化模型的代价函数。  

![](http://upload-images.jianshu.io/upload_images/5983416-143afdf7b2a2df38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

为了计算每一步的下降梯度，我们需要对这一亿条数据求和，这计算量是非常大的。因此，我们在事先应该分析我们需不需要这么大的数据集。在本例中，也许我们只用1000个数据也能得到较好的结果。在此期间，我们可以绘制学习曲线来帮助我们判断大数据集有没有必要。  

###### 随机梯度下降算法  

![](http://upload-images.jianshu.io/upload_images/5983416-42e994a0c22cd840.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在之前介绍的线性回归模型中，我们使用梯度下降算法最优化代价函数。在这小节中，我们依旧使用线性回归模型来介绍随机梯度下降算法。  

现在回想一下，我们之前所使用的梯度下降算法是如何运算的。

![](http://upload-images.jianshu.io/upload_images/5983416-da594d38c6558a21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，我们在每次更新参数θ时，算法都要对整个训练集遍历求和。我们将这种梯度下降算法称为批量梯度下降算法（Batch Gradient Descent Algorithm）。若训练集m的值非常大时，此时的计算代价就比较高了。  

因此，我们使用随机梯度下降算法（Stochastic Gradient Descent Algorithm）来解决该问题。在随机梯度下降算法中，先将训练集进行随机化处理，然后每完成一次计算就更新参数θ。  

![](http://upload-images.jianshu.io/upload_images/5983416-fef2c03293f07da2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

但随机梯度下降算法每次迭代并不意味着“正确”。因此，随机梯度下降算法可能最终都无法计算出全局最优值，其值实际上为接近全局最优值。  

###### 迷你批量梯度下降算法  

迷你批量梯度下降算法（Mini-Batch Gradient Descent Algorithm）是介于批量下降算法和随机梯度下降算法之间的梯度下降算法，其每计算b（b为常数）个训练实例，便更新一次参数θ。  

![](http://upload-images.jianshu.io/upload_images/5983416-4c8e3aaaa3c2d0b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，常数b的取值范围为2\~100。在这样的范围内，我们可以对训练集进行向量化处理。  

当对训练集向量化时，迷你梯度下降算法好于随机梯度下降算法。因为此时的迷你梯度下降算法能够实现并行运算，其运算速率相比随机梯度下降算法是要更快的。   

###### 随机梯度下降收敛  

在之前的学习中，我们通过绘制学习曲线来判断梯度下降算法是否收敛。因此，判断随机梯度下降算法是否收敛，我们仍然采用绘制学习曲线的方法。  

![](http://upload-images.jianshu.io/upload_images/5983416-e38c7c276c53ca8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  

其学习曲线如下：  

![](http://upload-images.jianshu.io/upload_images/5983416-97c0b88e3f0fcf90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，图中蓝色曲线均为最后1000个训练实例在随机梯度下降算法中的学习曲线。第一幅图，红色曲线为学习率α较小时，随机梯度下降算法的学习曲线；第二幅和第三幅图，红色曲线均为最后5000个训练实例在随机梯度下降算法中的学习曲线；第三幅图，紫红色曲线为最后5000个训练实例在随机梯度下降算法中的学习曲线，但其为异常曲线，我们需要调整学习率α或特征变量x；第四幅图，表明我们需要减小学习率α的值。    

![](http://upload-images.jianshu.io/upload_images/5983416-20e7628b7cbbb133.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

由于随机梯度下降算法所计算出的最优值实际上为局部最优值，因此为了进一步提升算法，我们也可以令学习率α的值随着迭代次数的增加而减小。如上图所示，例如令：  

![](http://upload-images.jianshu.io/upload_images/5983416-6dfb3f29825da3ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
