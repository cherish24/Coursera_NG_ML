支持向量机（Support Vector Machines，简称SVM）是一种二类分类模型。它在一些情况下，对于一些复杂的非线性问题能提供相比逻辑回归模型或神经网络模型更为简洁的解决方案。     

###### 优化目标  

我们通过不断改进逻辑回归模型以实现支持向量机。   

逻辑回归模型的假设函数h~θ~(x)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-961c1518d80b6fc3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中：   

- 当y = 1时，我们希望h~θ~(x) ≈ 1，即θ^T^x >> 0；  
- 当y = 0时，我们希望h~θ~(x) ≈ 0，即θ^T^x << 0；   

其代价函数J(θ)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-4075ec9ee27ae98b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

现假设在单个训练数据下，其代价函数J(θ)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-25faff29a7f66fe8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

将假设函数h~θ~(x)代入，则代价函数J(θ)为：  

![](http://upload-images.jianshu.io/upload_images/5983416-ca811dbe5bb80f3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于y = 1和y = 0这两种情况，我们可分别绘制出如下函数图：  

![](http://upload-images.jianshu.io/upload_images/5983416-c32192f74260b0bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
 
在上图基础上，我们分别取z = 1和z = -1两个点，绘制如下曲线：   

![](http://upload-images.jianshu.io/upload_images/5983416-289473e9f3f4af09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，图中紫红色曲线为支持向量机在y = 1和y = 0时的代价函数与变量z之间的函数关系图。       

现在我们以正则化的逻辑回归为例得出支持向量机的代价函数。  

正则化的逻辑回归的代价函数J(θ)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-ba044259faee6c15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

根据函数图我们可得支持向量机的代价函数J(θ)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-867ad8fe45423d42.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，C为一常数，类似于正则化的逻辑回归中的λ。   

同时，支持向量机的假设函数h~θ~(x)为：  

![](http://upload-images.jianshu.io/upload_images/5983416-2849b76f077e8169.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###### 初识最大间隔分类器（支持向量机）    

支持向量机的假设函数h~θ~(x)为：

![](http://upload-images.jianshu.io/upload_images/5983416-045f1325ebe17f03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

 但支持向量机要求更高，其不仅仅要正确分开输入的样本，即不仅仅要求θ^T^x ≥ 0（或θ^T^x ≤ 0），更要求θ^T^x ≥ 1（或θ^T^x ≤ -1），即额外添加了一个安全因子（或安全间距）。

因此，对于y = 1和y = 0这两个情况，可做如下改写：   

- 当y = 1时，θ^T^x ≥ 1   
- 当y = 0时，θ^T^x ≤ -1  

基于此，我们可将代价函数J(θ)改写为：  

![](http://upload-images.jianshu.io/upload_images/5983416-799c06035ef34cff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，此时常数C为一个非常大的数。    

此时，对于下图支持向量机会如何作出判定边界？    

![](http://upload-images.jianshu.io/upload_images/5983416-617c5a5b6d01ced2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

下图中的黑色线条即为支持向量机所作出的判定边界，它并不会以紫红色或绿色线条作为判定边界。这是因为支持向量机会尝试找到一个与样本之间有着最大间隔的判定边界，因此支持向量机也称为最大间隔分类器。

![](http://upload-images.jianshu.io/upload_images/5983416-08ec834a7c240199.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)        

最后，我们来讨论一下常数C的取值。    

![](http://upload-images.jianshu.io/upload_images/5983416-048df39bb0da1ad4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

从上图中，我们可以发现常数C的值若非常大，则会考虑数据集中的异常数据；而常数C的值若适中，则会忽略数据集中的异常数据，得到一个较为合适的判定边界。  

当C = 1/λ 时：   

- 常数C 较大时，类似于λ 较小，可能会导致过拟合，即高方差  
- 常数C 较小时，类似于λ 较大，可能会导致低拟合，即高偏差    

###### 最大间隔分类器背后的数学知识   

![](http://upload-images.jianshu.io/upload_images/5983416-52bd326073750786.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中，支持向量机的代价函数为：  

![](http://upload-images.jianshu.io/upload_images/5983416-676b2c7df168dd58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

假设θ~0~ = 0且n = 2，根据向量內积的相关数学知识，我们可推得：   

![](http://upload-images.jianshu.io/upload_images/5983416-a4001a297a154f02.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

以及推得θ^T^x^(i)^ = p^(i)^||θ||。    

![](http://upload-images.jianshu.io/upload_images/5983416-e3287946970ca73a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图中，绿线表示支持向量机作出的判定边界，蓝线表示与判定边界正交的参数θ向量。     

根据之前向量內积的相关数学知识，我们可以清楚了解到支持向量机如何作出与样本之间保持最大间隔的判定边界。            

在本例中，我们设置了θ~0~ = 0，这保证了判定边界过原点。   

注：常数C为一个非常大的数。
