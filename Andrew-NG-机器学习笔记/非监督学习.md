###### 聚类（Clustering）  

> 在非监督学习中，训练样本的标记信息是未知的，目标通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。此类学习任务中研究最多、应用最广的是聚类（Clustering）。  

> 聚类试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个簇（Cluster）。通过这样的划分，每个簇可能对应一些潜在的概念（类别）。需说明的是，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握命名。     

由于在课程中，吴恩达教授对这些概念没有明确定义。故从周志华教授的《机器学习》一书中摘取相关部分。  

###### K均值算法（K-means algorithm）  

K均值算法是聚类算法中最常用的算法，该算法能将一个无标记的数据集聚类成不同的簇。  

K均值算法是一个迭代算法，现假设训练集为{x^(1)^, x^(2)^, ... , x^(m)^}，即x^(i)^ ∈ R^n^；K表示簇的个数，则K均值算法的运行步骤如下：    

1. 随机选择K个点作为聚类中心，记为μ~1~, μ~2~, ... , μ~k~，其中μ~i~ ∈R^n^；   
2. 对于训练集中的每一个数据，按照其至聚类中心的距离的大小划分，将离某一个聚类中心距离最小的一系列数据归为一类；  
3. 计算每个簇的平均值并将聚类中心移至平均值处；  
4. 重复步骤2\~4直至聚类中心不再移动。  

![](http://upload-images.jianshu.io/upload_images/5983416-480c65f98c1ba3ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)   

###### 优化目标（Optimization Objective）  

K均值算法最小化问题是将数据点与其所属的聚类中心之间的距离最小化，因此其代价函数J(θ)为：  

![](http://upload-images.jianshu.io/upload_images/5983416-68c9b531f87c2e33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中μ~c^(i)~表示与数据点x^(i)^距离最小的聚类中心。该代价函数也称为Distortion Function。  

最小化代价函数的表达式为：  

![](http://upload-images.jianshu.io/upload_images/5983416-6774b2540e356e9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在最小化代价函数的过程中，每一次迭代都在使得代价函数值减小，否则其便是出现了错误。  

###### 随机初始化（Random Initialization）    

在使用K均值算法之前，我们应随机初始化K个聚类中心，其中聚类中心个数K要小于训练集数据个数m，即K < m。   

1. 随机选择K个训练集中的数据  
2. 将被选择的K个数据作为聚类中心  

![](http://upload-images.jianshu.io/upload_images/5983416-0bc2a7b52a9532bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

K均值算法在运行过程中可能会停留在一个局部最小值处，而这与初始化相关，因此我们要多次运行选择代价函数最小的结果。  

对于K值较小（如K = 2\~10）时，这种方法是可行的，其通过能够保证得到较好的局部最优解；但K值较大时，该方法效果不明显。     

###### 选择聚类个数（Choosing the Number of Cluster）  

实际上，我们并没有所谓最好的选择聚类个数的方法。我们通常是根据实际情况，人工选择聚类个数。
