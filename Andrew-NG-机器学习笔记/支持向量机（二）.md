###### Kernel Ⅰ  

![](http://upload-images.jianshu.io/upload_images/5983416-9ab677012edda9c7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于上图的数据集，我们之前使用的是多项式模型来对数据集进行分类操作。我们可能使用的模型为θ~0~ + θ~1~x~1~ + θ~2~x~2~ + θ~3~x~1~x~2~ + θ~4~x~1~^2^ + θ~5~x~2~^2^ + ...    

对于该模型当θ~0~ + θ~1~x~1~ + θ~2~x~2~ + θ~3~x~1~x~2~ + θ~4~x~1~^2^ + θ~5~x~2~^2^ + ... ≥ 0时，我们可以预测y = 1。因此，该模型的假设函数为：  

![](http://upload-images.jianshu.io/upload_images/5983416-beb9c881ddba66b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上述模型中，我们使用高阶项来更好地拟合数据集。但我们通常不清楚这些高阶项是否满足我们的需求，而且高阶项其计算量过大。因此，我们寻求使用新的特征变量来替代这些高阶项。例如：令f~1~ = x~1~，f~2~ = x~2~，f~3~ = x~1~x~2~，f~4~ = x~1~^2^，f~5~ = x~2~^2^，...，从而我们的模型变为θ~0~ + θ~1~f~1~ + θ~2~f~2~ + θ~3~f~3~ + θ~4~f~4~ + θ~5~f~5~ + ... 

除此之外，我们可以利用核函数（Kernel Function）来计算出新的特征。     

给定一个训练实例x，我们利用x的各个特征与我们预先选定的地标（landmark）l^(1)^，l^(2)^，l^(3)^距离大小来构造新的特征变量f~1~，f~2~，f~3~。   

![](http://upload-images.jianshu.io/upload_images/5983416-2ac13b6c46e3b671.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

通过使用核函数我们可以构建新的特征变量。  

![](http://upload-images.jianshu.io/upload_images/5983416-33c17e77a1069e5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中similarity(x, l)函数即为高斯核函数（Gaussian Kernel Function）。

其中  

![](http://upload-images.jianshu.io/upload_images/5983416-994203e8e22a9aa6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

表示实例x中所有特征与地标l^(1)^之间的距离的和。因此，我们以f~1~为例，将其改写为：  

![](http://upload-images.jianshu.io/upload_images/5983416-51ce61ac2d8ede91.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们根据x与地标距离的大小，可得出如下结论：  

![](http://upload-images.jianshu.io/upload_images/5983416-bd2265cfcee0b910.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中，关于σ^2^的取值，我们可以根据下图得出相关结论。  

![](http://upload-images.jianshu.io/upload_images/5983416-eb21f69721d2bd49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

即σ^2^的值越大，特征变量f~i~的变化曲线就越平滑，模型出现低方差高偏差问题；σ^2^的值越小，特征变量f~i~变化曲线就越陡峭，模型出现高方差低偏差问题。     

假设对于下图数据集，我们使用的模型为θ~0~ + θ~1~f~1~ + θ~2~f~2~ + θ~3~f~3~，当其大于等于零时，我们可以预测y = 1，其中θ~0~ = -0.5，θ~1~ = 1，θ~2~ = 1，θ~3~ = 0。

![](http://upload-images.jianshu.io/upload_images/5983416-b278b497c6684702.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中紫红色x与地标l^(1)^的距离小，因此我们可以认为f~1~ ≈ 1；而x与地标l^(2)^和l^(3)^距离较大，我们可以认为f~2~ ≈ 0，f~3~ ≈ 0。从而我们可以计算出θ~0~ + θ~1~f~1~ + θ~2~f~2~ + θ~3~f~3~ = 0.5 ≥ 0，因此我们可以预测在紫红色x处y = 1。同理，我们可以预测天蓝色x处y = 0，绿色x处y = 1。因此，我们可以画出红色线条表示的判定边界，其内的我们都可以预测y = 1，其外则y = 0。     

###### Kernel Ⅱ   

给定一个数据集为(x^(1)^, y^(1)^)，(x^(2)^, y^(2)^)，···，(x^(m)^, y^(m)^)，我们令l^(1)^ = x^(1)^，l^(2)^ = x^(2)^，···，l^(m)^ = x^(m)^。通常我们利用上述方法选择相应的地标，然后我们再利用核函数构建新的特征变量f~1~，f~2~，···，f~m~。因此，我们可将代价函数J(θ)改写为：   

![](http://upload-images.jianshu.io/upload_images/5983416-75bb8d03abe840ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
