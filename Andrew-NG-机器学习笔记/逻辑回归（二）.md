###### 代价函数（Cost Function）  

对于线性回归模型，我们定义的代价函数J(θ)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-500a6fd82adb5ca5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

现在对于逻辑回归模型我们沿用此定义，但问题是h~θ~(x) = g(z)，而函数g为S形函数，故代价函数J(θ)将会变为像下图中左边的图那样，此时我们将其称之为非凸函数（non-convex function）。    

![](http://upload-images.jianshu.io/upload_images/5983416-1cc44894bf76e208.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注：国外凸函数定义与国内是相反的。

这意味着代价函数J(θ)存在无数个局部最小值，从而影响梯度下降算法的运行。为了将代价函数J(θ)变为像上图中右边的图那样，我们想要将代价函数J(θ)重新定义为：      

![](http://upload-images.jianshu.io/upload_images/5983416-b214f19c95493f52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中   

![](http://upload-images.jianshu.io/upload_images/5983416-d129552c1abbbf55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

h~θ~(x)与Cost(h~θ~(x), y)之间的函数关系如下图所示：    

![](http://upload-images.jianshu.io/upload_images/5983416-9a77c530ff6aa0eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**y=1：**    
- h~θ~(x) -> 1，则Cost(h~θ~(x), y) = 0；    
- h~θ~(x) -> 0，则Cost(h~θ~(x), y) -> ∞。    

**y=0：**    
- h~θ~(x) -> 0，则Cost(h~θ~(x), y) = 0；    
- h~θ~(x) -> 1，则Cost(h~θ~(x), y) -> ∞。    

###### 补充笔记    

###### Cost Function    

We cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function.    

Instead, our cost function for logistic regression looks like:    

![](http://upload-images.jianshu.io/upload_images/5983416-b4c55680651d77b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

When y = 1, we get the following plot for J(θ) vs h~θ~(x):   

![](http://upload-images.jianshu.io/upload_images/5983416-9f9b9116f99ba547.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Similarly, when y = 0, we get the following plot for J(θ) vs h~θ~(x):        

![](http://upload-images.jianshu.io/upload_images/5983416-cbd202db1a177bcd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](http://upload-images.jianshu.io/upload_images/5983416-7b45a63cd0e618df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.  

If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.  

Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.         

###### 梯度下降算法    

之前为了得到一个凸函数（国内为凹函数），我们重新定义代价函数J(θ)为：      

![](http://upload-images.jianshu.io/upload_images/5983416-5c38304f9c0fe20d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于Cost(h~θ~(x), y)函数而言，它在y=1和y=0时有不同的函数表达式。为此我们可以将其合并为如下表达式：    

![](http://upload-images.jianshu.io/upload_images/5983416-5d5475c95e7cd100.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

因此，代价函数J(θ)可以简化为：    

![](http://upload-images.jianshu.io/upload_images/5983416-00e5231bc93097b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

既然有了上述的代价函数代价函数J(θ)，我们就要使用梯度下降算法来求得参数θ使得代价函数J(θ)最小化。         

![梯度下降算法](http://upload-images.jianshu.io/upload_images/5983416-0e843a7478cf8134.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

为了方便推导出迭代表达式，我们假设在单个样本下的代价函数J(θ)为：   

![](http://upload-images.jianshu.io/upload_images/5983416-887e4a2ee8a019c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

则：    

![](http://upload-images.jianshu.io/upload_images/5983416-363c2c54eaf7fc6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中：  

![](http://upload-images.jianshu.io/upload_images/5983416-75d174b180578cdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

注：上述推导过程摘自bitcarmanlee博主的[logistic回归详解(二）：损失函数（cost function）详解](http://blog.csdn.net/bitcarmanlee/article/details/51165444)和[logistic回归详解（三）：梯度下降训练方法](http://blog.csdn.net/bitcarmanlee/article/details/51473567)这两篇文章。谢谢！

因此对于全体样本其梯度下降算法为：    

![](http://upload-images.jianshu.io/upload_images/5983416-25c6866d6611b693.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

虽然该表达式和线性回归模型下的梯度下降算法表达式相同，但其h~θ~(x)是不同的。逻辑回归模型下的假设函数h~θ~(x)为：    

![](http://upload-images.jianshu.io/upload_images/5983416-a4562e5264bc6158.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在使用梯度下降算法时，我们仍然推荐将参数θ和特征变量x向量化，其表达式为：   

![](http://upload-images.jianshu.io/upload_images/5983416-f8e1ac6d055e232d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

###### 补充笔记   

###### Simplified Cost Function and Gradient Descent    

We can compress our cost function's two conditional cases into one case:    

![](http://upload-images.jianshu.io/upload_images/5983416-48cfb37c87f737a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Notice that when y is equal to 1, then the second term (1−y)log⁡(1−h~θ~(x)) will be zero and will not affect the result. If y is equal to 0, then the first term −ylog⁡(h~θ~(x)) will be zero and will not affect the result.     

We can fully write out our entire cost function as follows:    

![](http://upload-images.jianshu.io/upload_images/5983416-6d473355db818d7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

A vectorized implementation is:    

![](http://upload-images.jianshu.io/upload_images/5983416-d1c6fc3760371472.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**Gradient Descent**    

Remember that the general form of gradient descent is:    

![](http://upload-images.jianshu.io/upload_images/5983416-b56abd666a576e4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

We can work out the derivative part using calculus to get:    

![](http://upload-images.jianshu.io/upload_images/5983416-cba36fbbd62ef51e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta.   

A vectorized implementation is:     

![](http://upload-images.jianshu.io/upload_images/5983416-8b0ed0c70fff2c5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)      

###### 高级优化     

除了我们之前学习的梯度下降算法之外，还有一些高级的算法可以替代梯度下降算法使得代价函数J(θ)最小化，例如：共轭梯度算法（Conjugate Gradient）、局部优化算法（Broyden fletcher goldfarb shann,BFGS）和有限内存局部优化算法（LBFGS）。           

在Octave或MATLAB中，我们可以使用fminunc()函数，该函数可以用来求解无约束非线性问题的最小值。          

例如：

![](http://upload-images.jianshu.io/upload_images/5983416-11315029364dd9b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如上图所示，我们假设代价函数J(θ) = (θ~1~ - 5)^2^ + (θ~2~ - 5)^2^，现我们希望求出参数θ的值使得代价函数J(θ)最小化。          

首先，我们创建名为costFunction.m的文件并添加如下代码：       

```
function [jVal, gradient] = costFunction(theta)

jVal = (theat(1) - 5)^2 + (theta(2) - 5)^2;

gradient = zeros(2, 1);
gradient(1) = 2 * (theta(1) - 5);
gradient(2) = 2 * (theta(2) - 5);
```         

代码注释为：     

![](http://upload-images.jianshu.io/upload_images/5983416-7b21557c117a629b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

然后，我们在Octave中键入如下命令：      

```
octave:3> options = optimset('GradObj', 'on', 'MaxIter', '100');
octave:4> initialTheta = zeros(2, 1);
octave:5> [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options)
optTheta =

   5.0000
   5.0000

functionVal =   1.5777e-030
exitFlag =  1
```         

其中fminunc函数中@表示指向costFunction.m文件中costFunction函数的指针，exitFlag =  1表示函数已经收敛找到了使得代价函数J(θ)最小化的参数θ的值。      

补充说明（此处谢谢[Giacche](http://www.jianshu.com/u/4cb74eda19dd)在评论区的补充）：      

fminunc表示Octave里无约束最小化函数，调用这个函数时，需要传入一个存有配置信息的变量options。     

```
options = optimset('GradObj', 'on', 'MaxIter', '100');
```    

上面的代码中，我们的设置项中’GradObj’, ‘on’,代表设置梯度目标参数为打开状态(on)。’MaxIter’，‘100’代表设置最大迭代次数为100次。    

```
initialTheta = zeros(2,1);
```   

initialTheta代表我们给出的一个θ的猜测初始值。

当我们调用这个fminunc函数时，它会自动的从众多高级优化算法中挑选一个来使用(你也可以把它当做一个可以自动选择合适的学习速率aa的梯度下降算法)。     

最终我们会得到三个返回值，分别是满足最小化代价函数J(θ)的θ值optTheta，costFunction中定义的jVal的值functionVal，以及标记是否已经收敛的状态值exitFlag，如果已收敛，标记为1，否则为0。

###### 补充笔记       

###### Advanced Optimization     

"Conjugate gradient", "BFGS", and "L-BFGS" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they're already tested and highly optimized. Octave provides them.       

We first need to provide a function that evaluates the following two functions for a given input value θ:   

![](http://upload-images.jianshu.io/upload_images/5983416-8573b3c1441bd8e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

We can write a single function that returns both of these:    

```
function [jVal, gradient] = costFunction(theta)
  jVal = [...code to compute J(theta)...];
  gradient = [...code to compute derivative of J(theta)...];
end
```    

Then we can use octave's "fminunc()" optimization algorithm along with the "optimset()" function that creates an object containing the options we want to send to "fminunc()".         

```
options = optimset('GradObj', 'on', 'MaxIter', 100);
initialTheta = zeros(2,1);
   [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
```          

We give to the function "fminunc()" our cost function, our initial vector of theta values, and the "options" object that we created beforehand.
